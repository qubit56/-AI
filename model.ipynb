{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S89AJpQYG3du"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlnQdsrDh9AX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (60000, 28, 28, 1), y_train.shape = (60000, 10)\n",
      "x_test.shape = (10000, 28, 28, 1), y_test.shape = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data(path='mnist.npz')\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "print(\"x_train.shape = {}, y_train.shape = {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"x_test.shape = {}, y_test.shape = {}\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lq0YDUYiTMN"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28, 28, 1), name='input')\n",
    "\n",
    "x = Conv2D(24, kernel_size=(6, 6), strides=1)(inputs)\n",
    "x = BatchNormalization(scale=False, beta_initializer=Constant(0.01))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "\n",
    "x = Conv2D(48, kernel_size=(5, 5), strides=2)(x)\n",
    "x = BatchNormalization(scale=False, beta_initializer=Constant(0.01))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(4, 4), strides=2)(x)\n",
    "x = BatchNormalization(scale=False, beta_initializer=Constant(0.01))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(200)(x)\n",
    "x = BatchNormalization(scale=False, beta_initializer=Constant(0.01))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "\n",
    "predications = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predications)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBzYWAEAiwzx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 23, 23, 24)        888       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 23, 23, 24)        72        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 23, 23, 24)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 23, 23, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 48)        28848     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 10, 48)        144       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10, 10, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 10, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          49216     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 64)          192       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               205000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               600       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 286,970\n",
      "Trainable params: 286,298\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McycGoh0itz2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0201.\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 91s 2ms/sample - loss: 0.1598 - accuracy: 0.9510 - val_loss: 0.0551 - val_accuracy: 0.982565\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.014430626211475785.\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 94s 2ms/sample - loss: 0.0649 - accuracy: 0.9803 - val_loss: 0.0345 - val_accuracy: 0.9897\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01036834238065184.\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 104s 2ms/sample - loss: 0.0431 - accuracy: 0.9868 - val_loss: 0.0289 - val_accuracy: 0.9912\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.007457588823428847.\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.0344 - accuracy: 0.9893 - val_loss: 0.0178 - val_accuracy: 0.9940\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.005371942762314537.\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 107s 2ms/sample - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.0174 - val_accuracy: 0.9939\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0038775120567512366.\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 105s 2ms/sample - loss: 0.0235 - accuracy: 0.9927 - val_loss: 0.0190 - val_accuracy: 0.9942\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.002806705664732254.\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.0183 - accuracy: 0.9940 - val_loss: 0.0182 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.002039439357288101.\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.0160 - accuracy: 0.9948 - val_loss: 0.0160 - val_accuracy: 0.9949\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0014896690244560313.\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.0155 - accuracy: 0.9950 - val_loss: 0.0143 - val_accuracy: 0.9948\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001095741367357279.\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.0136 - accuracy: 0.9955 - val_loss: 0.0148 - val_accuracy: 0.9947\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.000813479866945048.\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.0119 - accuracy: 0.9963 - val_loss: 0.0151 - val_accuracy: 0.9950\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0006112306641301482.\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 94s 2ms/sample - loss: 0.0116 - accuracy: 0.9961 - val_loss: 0.0146 - val_accuracy: 0.9952\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00046631277777468366.\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.0132 - val_accuracy: 0.9953\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00036247457473881936.\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 90s 1ms/sample - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.0137 - val_accuracy: 0.9951\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.00028807125102990415.\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 122s 2ms/sample - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.0136 - val_accuracy: 0.9951\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0002347589399817094.\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 132s 2ms/sample - loss: 0.0101 - accuracy: 0.9966 - val_loss: 0.0137 - val_accuracy: 0.9954\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00019655899987662886.\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 91s 2ms/sample - loss: 0.0096 - accuracy: 0.9966 - val_loss: 0.0137 - val_accuracy: 0.9954\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001691875467292952.\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 91s 2ms/sample - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.0137 - val_accuracy: 0.9957\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001495750435333272.\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 91s 2ms/sample - loss: 0.0087 - accuracy: 0.9971 - val_loss: 0.0134 - val_accuracy: 0.9955\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0001355220709146876.\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 92s 2ms/sample - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0136 - val_accuracy: 0.9953\n"
     ]
    }
   ],
   "source": [
    "lr_decay = lambda epoch: 0.0001 + 0.02 * math.pow(1.0 / math.e, epoch / 3.0)\n",
    "decay_callback = LearningRateScheduler(lr_decay, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, \n",
    "                    validation_data=(x_test, y_test), callbacks=[decay_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FinX93e1jKz9"
   },
   "outputs": [],
   "source": [
    "model.save('mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioCGtGB3mkDv",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TFLiteConverterV2' has no attribute 'from_keras_model_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_16108/2693626263.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_keras_model_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist.tflite'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtflite_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'TFLiteConverterV2' has no attribute 'from_keras_model_file'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file('mnist.h5')\n",
    "tflite_model = converter.convert()\n",
    "open('mnist.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TFLiteConverterV2' has no attribute 'from_keras_model_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_16108/137157266.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 저장한 파일로부터 모델 변환 후 다시 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_keras_model_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist.tflite'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtflite_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'TFLiteConverterV2' has no attribute 'from_keras_model_file'"
     ]
    }
   ],
   "source": [
    "# 저장한 파일로부터 모델 변환 후 다시 저장\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file('mnist.h5')\n",
    "tflite_model = converter.convert()\n",
    "with open('mnist.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "#save_model('./model/mnist.h5', './model/mnist.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51PTkdoPDOTW"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('mnist.tflite')\n",
    "except:\n",
    "    print(\"Skip downloading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\tmps9e8j9bl\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1147380"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WHOLE MODEL\n",
    "tflite_model = tf.keras.models.load_model('mnist.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(tflite_model)\n",
    "tflite_save = converter.convert()\n",
    "open(\"mnist.tflite\", \"wb\").write(tflite_save)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mnist_batchnorm.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
